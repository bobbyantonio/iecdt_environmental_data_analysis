{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Big Data \n",
    "\n",
    "Here we will discuss tools to deal with `Big Data`. What counts as Big Data depends on the resources at hand, but people often seem to define it as 'Medium' data as when it can't fit in RAM, and Big Data when it can't fit in RAM and when it takes up a lot of disk space (e.g. on the order of Terabytes)\n",
    "\n",
    "To deal with Medium or big data, there are useful libraries that enable parralelisation of tasks form numpy and xarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import xarray as xr\n",
    "import dask\n",
    "from pathlib import Path\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "data_dir = str(Path(notebook_dir).parents[1] / 'data_samples' )\n",
    "WEATHERBENCH_BUCKET = 'gs://weatherbench2/datasets/era5/1959-2022-full_37-6h-0p25deg_derived.zarr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask is one of many computing libraries that uses 'lazy' evaluation to deal with large amounts of data. The key concept is that, rather than evaluating everything immediately, the program waits until explicitly asked to compute everything, and then it will try to make the computation as efficient as possible\n",
    "\n",
    "Resources: \n",
    "https://tristansalles.github.io/EnviReef/6-addson/dask.html, \n",
    "https://docs.xarray.dev/en/stable/user-guide/dask.html\n",
    "\n",
    "https://tutorial.xarray.dev/intermediate/xarray_and_dask.html\n",
    "\n",
    "Let's start by creating a basic dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "ones = da.ones((2000, 5000))\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the size of each chunk of data using chunks\n",
    "darray = da.ones((2000, 5000), chunks=(1000, 1000))\n",
    "darray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can perform operations on this array, but it won't actually do anything, just makes a note of the operations, and creates a graph\n",
    "total_sum = darray.sum()\n",
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To actually get the answer, we can finally call compute\n",
    "total_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the xarray documentation\n",
    "\n",
    "''A good rule of thumb is to create arrays with a minimum chunksize of at least one million elements (e.g., a 1000x1000 matrix). With large arrays (10+ GB), the cost of queueing up Dask operations can be noticeable, and you may need even larger chunksizes.''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a dataset with dask\n",
    "\n",
    "Most of the time, we don't have to interact with dask directly, because xarray has great integration with dask.\n",
    "\n",
    "We can simply specify the size of the chunks when opening a netcdf file, and under the hood it will store the data as a dask array\n",
    "\n",
    "Simply specify how many chunks to separate your dimensions into. Since latitude and longitude aren't specified, the default is to have one chunk for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(os.path.join(data_dir, 'netcdf', 'E-OBS', 'UK_monthly.nc'), chunks={\"time\": 200, 'latitude': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the data hasn't been loaded yet, it is a dask array, and just shows details of the data shape\n",
    "ds['pp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also do this by loading it normally and using the .chunks() method\n",
    "\n",
    "ds = xr.open_dataset(os.path.join(data_dir, 'netcdf', 'E-OBS', 'UK_monthly.nc'))\n",
    "ds = ds.chunk({'time': 200})\n",
    "ds['pp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform operations like on a regular xarray Dataset / Dataarray\n",
    "# Nearly all xarray operations have been extended so that they are compatible with dask\n",
    "\n",
    "final_ds = ds.isel(time=slice(0,100)).groupby('time.year').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at what operations the dataset will do. Note that only one chunk of the data is being acted on,\n",
    "# Because of the time values we have chosen\n",
    "dask.visualize(final_ds.pp.data, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally when you are ready, call .compute() or .load() to make the computation actually happen\n",
    "# The difference between load and compute: load operates in-place (i.e. the original dataset is overwritten) whereas compute returns the loaded dataset without overwriting the original\n",
    "\n",
    "# so we can do this\n",
    "computed_ds = final_ds.compute()\n",
    "\n",
    "# Or alternatively overwrite final_ds with the in-memory dataset\n",
    "final_ds.load()\n",
    "\n",
    "# Alternatively, you can write the dataset to file, without having to load into memory, by just saving using ds.to_netcdf(\"filename.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access the data, calling .values will force the data into memory, whilst .data will keep it as a dask array\n",
    "# So using .data can be useful if you have non-standard functions that act on your data\n",
    "print(type(ds['pp'].values))\n",
    "print(type(ds['pp'].data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multifile datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we will look at opening a dataset made up of many files, which is a common occurence. One way of doing this is to just loop over the files yourself, and combine them using e.g. `concat`. But there is also a handy function to do this in one step in xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a new multifile dataset in a new temporary directory \n",
    "uk_monthly_ds = xr.open_dataset(os.path.join(data_dir, 'netcdf', 'E-OBS', 'UK_monthly.nc'))\n",
    "\n",
    "# Create a temporary directory, if it doesn't already exist\n",
    "temp_dir_path = os.path.join(data_dir, 'tmp')\n",
    "os.makedirs(temp_dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "for n, t in enumerate(uk_monthly_ds.time.values):\n",
    "\n",
    "    tmp_ds = uk_monthly_ds.sel(time=t)\n",
    "\n",
    "    tmp_ds.expand_dims(time=1).to_netcdf(os.path.join(temp_dir_path, f\"uk_monthly_{n}.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load it easily using open_mfdataset\n",
    "# First find all the relevant filepaths. There are many ways to do this, including a manual loop, but often glob is handy, as it uses a linux-like syntax\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "# Find all filepaths that are of the form uk_monthly_*.nc (where * is a wildcard, so can be any character)\n",
    "relevant_fps = glob(os.path.join(temp_dir_path, 'uk_monthly_*.nc'))[:20]\n",
    "\n",
    "print(relevant_fps[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now pass this list of filepaths into open_mfdataset\n",
    "mutifile_dataset = xr.open_mfdataset(relevant_fps, \n",
    "                                     combine='nested', \n",
    "                                     concat_dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has created us a dataset made up of dask arrays\n",
    "# Note by default it has chunked the data into one chunk per time step\n",
    "mutifile_dataset['pp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to specify a particular chunking for the data, then it's best to specify the chunks when opening the data, rather than loading with the default\n",
    "mutifile_dataset = xr.open_mfdataset(relevant_fps, \n",
    "                                     combine='nested', \n",
    "                                     concat_dim='time', \n",
    "                                     parallel=True, \n",
    "                                     chunks={'time': 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using zarr arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zarr is a common format for storing large multi-dimensional arrays data, often used for storing data in the cloud.\n",
    "\n",
    "The data is stored in a distributed (chunked) and compressed format\n",
    "\n",
    "Often we can access zarr data using xarray, but not always, as the zarr datahas to be saved in a particular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One example is the data stored as part of the Weatherbench project. This is ERA5 reanalysis data stored as zarr files\n",
    "# e.g. we can load ERA5 data from weatherbench\n",
    "ds = xr.open_zarr(WEATHERBENCH_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is a dataset where data is stored as dask arrays\n",
    "ds['2m_temperature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above, we can then subselect the data, and eventually call compute() to fetch the data. In this case, it's the 2-metre temperature on 1st January 2016 at 00:00am  a particular point. \n",
    "\n",
    "ds['2m_temperature'].sel(latitude=0).sel(longitude=0).sel(time=datetime.datetime(2016,1,1,0)).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising computations using the dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# This piece of code is just for a correct dashboard link mybinder.org or other JupyterHub demos\n",
    "import dask\n",
    "import os\n",
    "\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just click on the Dashboard link above to access the dashboard\n",
    "\n",
    "Then try running the below code and it will show you information about the run, such as CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.open_mfdataset(relevant_fps, \n",
    "                                     combine='nested', \n",
    "                                     concat_dim='time', \n",
    "                                     chunks={'time': 4}).groupby('time.year').mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we need to close the client\n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
